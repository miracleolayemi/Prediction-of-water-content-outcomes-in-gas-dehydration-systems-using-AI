
#Libraries imported
import pandas as pd
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, recall_score, f1_score, precision_score

np.random.seed(42)

pd.set_option('display.max_rows',None,'display.max_columns',None,'display.width',None,'display.max_colwidth',-1)


#To load the data set and specify the input features
df=read_csv('training_data_for_ANN_prediction.csv')
X,y=df.values[:,:-1],df.values[:,-1]
X=X.astype('float32')

#The Model function
def FFNN(X_train, y_train):
    #Model definition
    n_features=X_train.shape[1]
    model=Sequential()
    model.add(Dense(7,activation='relu',kernel_initializer='he_normal',input_shape=(n_features,)))
    model.add(Dense(1,activation='sigmoid'))

    #Model compilation
    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

    #Model fitting
    history= model.fit(X_train,y_train,epochs=150,batch_size=32, validation_split=0.2, verbose=0)

    #To get the output of the model fitting
    history_dict=history.history
    #print(history_dict.keys())

    #To plot the model accuracy using training data and validation data
    plt.plot(history_dict['accuracy'])
    plt.plot(history_dict['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['train', 'val'], loc='upper left')
    plt.show()


    #To plot the model loss using training data and validation data
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['train', 'val'], loc='upper left')
    plt.show()

    #To get the training and validation data accuracy
    training_accuracy_history=history_dict['accuracy']
    Training_Accuracy= training_accuracy_history[-1]

    validation_accuracy_history=history.history['val_accuracy']
    Validation_Accuracy=validation_accuracy_history[-1]

    #To get the training and validation data loss
    training_loss_history=history_dict['loss']
    Training_Loss= training_loss_history[-1]

    validation_loss_history=history_dict['val_loss']
    Validation_loss= validation_loss_history[-1]

    print("Training Accuracy: %.3f" % Training_Accuracy)
    print("Validation Accuracy: %.3f" % Validation_Accuracy)
    print("Training Loss: %.3f" % Training_Loss)
    print("Validation loss: %.3f" % Validation_loss)

    return model


#To split the data into training, validation, and testing
X_train,X_test,y_train, y_test=train_test_split(X,y,test_size=0.2)

#To standardize the data
trans=StandardScaler()
X_train=trans.fit_transform(X_train)

model=FFNN(X_train,y_train)


#The test data evaluation function
def test_evaluation(model, X_test,y_test):
    loss,acc=model.evaluate(X_test,y_test,verbose=2)
    print('Test Accuracy: %.3f' % acc )
    print('Test loss: %.3f' % loss )

    #Predicting the test set values
    y_predict=model.predict_classes(X_test)

    #Making the confusion_matrix
    cm=confusion_matrix(y_test,y_predict)
    print('Test CM:',cm)

    #Getting the precision score
    pscore=precision_score(y_test,y_predict)
    print('Test PS:',pscore)

    #Getting the recall score
    rscore=recall_score(y_test,y_predict)
    print('Test RS:',rscore)

    #Getting the f1score
    f1score=f1_score(y_test,y_predict)
    print('Test F1S:',f1score)


#To standardize X_test
X_test=trans.transform(X_test)
test_evaluation(model, X_test,y_test)

#
#To evaluate model with experimental data (Arubi,2008)
#To load the data
arubi=read_csv('arubi2008_data.txt')

#To vectorize and specify the input features
arubi_X,arubi_y=arubi.values[:,:-1],arubi.values[:,-1]

#To standardize the data
arubi_X=trans.transform(arubi_X)

#The experimental data evaluation function
def exp_data_evaluation(model, arubi_X, arubi_y):

    #To evaluate the model
    arubi_loss,arubi_acc=model.evaluate(arubi_X,arubi_y,verbose=0)
    print('Arubi Accuracy: %.3f' % arubi_acc)
    print('Arubi loss: %.3f' % arubi_loss)

    #To predict the arubi2008 data
    arubi_y_predict= model.predict_classes(arubi_X)

    #Making the confusion matrix for arubi2008
    arubi_cm=confusion_matrix(arubi_y,arubi_y_predict)
    print('Arubi Confusion matrix:',arubi_cm)

    #Getting the preecision score, recall score ,and F1 score for arubi2008
    arubi_pscore=precision_score(arubi_y,arubi_y_predict)
    print('Arubi Precision score:',arubi_pscore)

    arubi_rscore=recall_score(arubi_y,arubi_y_predict)
    print('Arubi Recall score:',arubi_rscore)

    arubi_fscore=f1_score(arubi_y,arubi_y_predict)
    print('Arubi F1 score: %.3f' % arubi_fscore)

exp_data_evaluation(model, arubi_X, arubi_y)

# #For new predictions-
# process_parameters=[TEG_recirc_rate, no_of_equili_stages, temp, stripping_rate]
#[2.9,1,360,0] #offspec example ( class 1)
#[3.6,2,400,0] #on-spec example (class 0)

#The new predictions function
def new_predictions(model,process_parameters):

    #Data preparation
    process_parameters=np.array(process_parameters)
    process_parameters=process_parameters.reshape(1,-1)
    process_parameters=process_parameters.astype('float32')
    process_parameters=trans.transform(process_parameters)

    #Prediction
    new_prediction_output=model.predict(process_parameters)
    new_prediction_class=model.predict_classes(process_parameters)

    #Prediction output
    if new_prediction_class==0:
        print('Predicted: %.3f' % new_prediction_output)
        print('Predicted Class: %.3f' % new_prediction_class)
        print("Pipeline quality gas meets water content specification!")

    elif new_prediction_class==1:
        print('Predicted: %.3f' % new_prediction_output)
        print('Predicted Class: %.3f' % new_prediction_class)
        print("Pipeline quality gas does not meet water content specification!")

process_parameters= [2.9,1,360,0]
new_predictions(model,process_parameters)
